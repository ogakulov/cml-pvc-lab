{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook does some basic **exploratory data analysis** (EDA) of the telco churn data.\n",
    "Here, we get a qualitative and a quantitative sense of the data,\n",
    "what kind of cleanup it might need before we can use it,\n",
    "and if there are any specific patterns that can be discerned.\n",
    "\n",
    "If you haven't yet, run through the initialization steps in the README file and Part 1.\n",
    "In Part 1, the data is imported into the table you specified in Hive. All new data accesses will fetch from Hive if available, otherwise the local copy of the data will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We start by creating a `SparkSession` to fetch the data using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "hive_database = os.environ['HIVE_DATABASE']\n",
    "hive_table = os.environ['HIVE_TABLE']\n",
    "hive_table_fq = hive_database + '.' + hive_table\n",
    "\n",
    "# connect to Spark\n",
    "spark = SparkSession.builder.appName(\"Telco Data Set\").master(\"local[*]\")\\\n",
    "        .config(\"spark.sql.catalog.local.type\",\"hadoop\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "# read data into a Spark DataFrame\n",
    "if os.environ[\"STORAGE_MODE\"] == \"external\":\n",
    "    telco_data_raw = spark.sql(\"SELECT * FROM \" + hive_table_fq)\n",
    "else:\n",
    "    path = \"/home/cdsw/raw/WA_Fn-UseC_-Telco-Customer-Churn-.csv\"\n",
    "    schema = StructType(\n",
    "                        [\n",
    "                            StructField(\"customerID\", StringType(), True),\n",
    "                            StructField(\"gender\", StringType(), True),\n",
    "                            StructField(\"SeniorCitizen\", StringType(), True),\n",
    "                            StructField(\"Partner\", StringType(), True),\n",
    "                            StructField(\"Dependents\", StringType(), True),\n",
    "                            StructField(\"tenure\", DoubleType(), True),\n",
    "                            StructField(\"PhoneService\", StringType(), True),\n",
    "                            StructField(\"MultipleLines\", StringType(), True),\n",
    "                            StructField(\"InternetService\", StringType(), True),\n",
    "                            StructField(\"OnlineSecurity\", StringType(), True),\n",
    "                            StructField(\"OnlineBackup\", StringType(), True),\n",
    "                            StructField(\"DeviceProtection\", StringType(), True),\n",
    "                            StructField(\"TechSupport\", StringType(), True),\n",
    "                            StructField(\"StreamingTV\", StringType(), True),\n",
    "                            StructField(\"StreamingMovies\", StringType(), True),\n",
    "                            StructField(\"Contract\", StringType(), True),\n",
    "                            StructField(\"PaperlessBilling\", StringType(), True),\n",
    "                            StructField(\"PaymentMethod\", StringType(), True),\n",
    "                            StructField(\"MonthlyCharges\", DoubleType(), True),\n",
    "                            StructField(\"TotalCharges\", DoubleType(), True),\n",
    "                            StructField(\"Churn\", StringType(), True),\n",
    "                        ]\n",
    "                    )\n",
    "    telco_data_raw = spark.read.csv(path, header=True, sep=\",\", schema=schema, nullValue=\"NA\")\n",
    "\n",
    "# print a few rows\n",
    "telco_data_raw.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis using Spark DataFrames\n",
    "\n",
    "Spark Dataframes essentially allow you to express SQL-like statements with the horizontal scalability of Spark. \n",
    "You can perform your familiar filter, count, groupby etc. \n",
    "\n",
    "For more, see the [official Spark Documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html).\n",
    "\n",
    "We demonstrate some high-level stats and basic aggregations you might do to get an initial sense of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of customers in the dataset\n",
    "telco_data_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of customers by internet service type \n",
    "telco_data_raw.groupby(\"InternetService\")\\\n",
    "              .count()\\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Temporary Spark Table\n",
    "\n",
    "Using the `createOrReplaceTempView` functions allows you register a temporary table (called a view) in Spark.\n",
    "This lets you use SQL syntax to make the queries instead of the method chaining we used in the last statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_data_raw.createOrReplaceTempView(\"telco\")\n",
    "spark.sql(\"SELECT Churn, count(*) FROM telco GROUP BY Churn\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are some null values in the `Churn` column,\n",
    "which is the label column telling us whether the customer churned or not.\n",
    "We'll have to clean the rows with those nulls out since we don't know their true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_data = telco_data_raw.filter(telco_data_raw.Churn.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_customers = telco_data.count()\n",
    "churned_customers = telco_data.filter(telco_data.Churn == 'Yes').count()\n",
    "remain_customers = telco_data.filter(telco_data.Churn == 'No').count()\n",
    "\n",
    "\"Customers total: {}, Churned : {}, Remained : {}\".format(total_customers, churned_customers, remain_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualisation\n",
    "\n",
    "Exploratory data analysis usually involves a fair bit of data visualization. \n",
    "The workflow for large data sets is usually:\n",
    "\n",
    "* [Take a sample](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample) of the data so it fits in memory on a single machine.\n",
    "* Examine single variable distributions.\n",
    "* Examine joint distributions and correlations.\n",
    "* Look for other types of relationships.\n",
    "\n",
    "We start with taking a 50% sample and saving it in a pandas DataFrame to feed into our visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: toPandas() => brings data locally into a Pandas DataFrame\n",
    "sample_data = telco_data.sample(withReplacement=False, fraction=0.5, seed=83).toPandas()\n",
    "sample_data.head().T  # transpose for easier reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature distributions (and installing packages)\n",
    "\n",
    "We want to examine the distribution of our features, so start with them one at a time.\n",
    "\n",
    "Let's use seaborn's [distplot() function](https://seaborn.pydata.org/generated/seaborn.distplot.html) to easily examine the distribution of a column.\n",
    "\n",
    "In the initialisation step when you run `0_bootstrap.py` file, the python requirements are installed. If you have not run that step yet, and seaborn is not installed, you can [**install it and other python packages**](https://docs.cloudera.com/machine-learning/cloud/engines/topics/ml-install-pkg-lib.html) now. \n",
    "\n",
    "> Navigate to your Project's **Overview** page. Click **Open Workbench** and launch a session.\n",
    "> Install using the `!` shell operator: \n",
    "> \n",
    "> `!pip3 install seaborn`. \n",
    "> \n",
    "> This is also how you would install from a `requirements.txt` file:\n",
    "> \n",
    "> `!pip3 install -r requirements.txt`\n",
    "> \n",
    "\n",
    "These packages will be available to all scripts and notebooks in the Project, \n",
    "similar to installing a repo's `requirements.txt` dependencies into a *virtualenv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.grid(b=None)\n",
    "plt.title(\"Tenure Distribution\",color='grey')\n",
    "plt.xticks(color='grey')\n",
    "plt.yticks(color='grey')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True,bottom=True)\n",
    "\n",
    "sns.distplot(sample_data['tenure'], kde=False, axlabel=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:640px;font-size:16px;background-color:WhiteSmoke;padding:15px\">\n",
    "\n",
    "We can examine feature differences in the distribution of our features when we condition (split) our data.\n",
    "We'll use [seaborn's BoxPlot function](https://seaborn.pydata.org/generated/seaborn.boxplot.html?highlight=boxplot#seaborn.boxplot) for this, \n",
    "though we could have also use many other functions or entirely different packages.\n",
    "\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.grid(b=None)\n",
    "plt.title(\"Tenure Box Plot\",color='grey')\n",
    "plt.xticks(color='grey')\n",
    "plt.yticks(color='grey')\n",
    "plt.xlabel(\"\",color='grey')\n",
    "plt.ylabel(\"\",color='grey')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine(left=True,bottom=True)\n",
    "\n",
    "sns.boxplot(x=\"Churn\", y=\"tenure\", data=sample_data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these examples, we only examined one column,\n",
    "but a complete analysis would examine all the rest for data quality issues or potentially useful or useless features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Distributions\n",
    "Looking at joint distributions of data can also tell us a lot, particularly about duplicated features that need removing.\n",
    "[Seaborn's PairPlot](https://seaborn.pydata.org/generated/seaborn.pairplot.html?highlight=pairplot#seaborn.pairplot) is a quick way to look at joint distributions for many variables at once.\n",
    "Note that it may struggle with a large number of columns, so we demonstrate with three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "example_numeric_data = sample_data[[\"MonthlyCharges\", \"TotalCharges\",\"tenure\",\"Churn\"]]  # pick a few columns\n",
    "sns.pairplot(example_numeric_data, hue=\"Churn\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop the Spark session\n",
    "Jupyter Notebooks take longer to timeout than regular workbench sessions, so its good practice to add a `spark.stop()` call in you notebook to make sure any spark resources are added back to the cluster if you aren't using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "We've now covered a quick **exploratory data analysis** using Spark SQL, Spark DataFrames, and seaborn.\n",
    "We've also explained how to **install packages** in a CML Project.\n",
    "This is just a taste, and you can install any of your favorite packages—even custom ones—to dig even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If this documentation includes code, including but not limited to, code examples, Cloudera makes this available to you under the terms of the Apache License, Version 2.0, including any required notices.  A copy of the Apache License Version 2.0 can be found [here](https://opensource.org/licenses/Apache-2.0).***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
